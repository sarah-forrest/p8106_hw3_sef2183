---
title: "P8106 Data Science II Homework 3: Predicting Gas Milage"
author: "Sarah Forrest - sef2183"
date: "3/24/2023"
output: github_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE, warning = FALSE, dpi = 300, fig.width = 7)
```

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(glmnet)
library(MASS)
library(pROC)
```

# Data

In this exercise, we build a model to predict whether a given car gets high or low gas mileage based on a set of predictors from the dataset “auto.csv”. The dataset contains 392 observations. The response variable is `mpg_cat`, which indicates whether the miles per gallon of a car is high or low. 

```{r}
# read in data
auto = read.csv("data/auto.csv") 
```

Split the dataset into two parts: training data (70%) and test data (30%):

```{r}
set.seed(1)

# specify rows of training data (70% of the dataset)
rowTrain <- createDataPartition(y = auto$mpg_cat, 
                              p = .7,
                              list = F)

```

Mutate the data so the outcome variable `mpg_cat` takes numeric values of 0 and 1 rather than character values "low" and "high" in order to run the `glm()` function:

```{r}
auto_glm = 
  auto %>%
  mutate(mpg_cat = case_when(
    mpg_cat == "low" ~ 0,
    mpg_cat == "high" ~ 1))
```

# (a) Perform a logistic regression using the training data.

```{r}
set.seed(1)

glm.fit <- glm(mpg_cat ~ .,
               data = auto_glm,
               subset = rowTrain,
               family = binomial(link = "logit"))

summary(glm.fit)
```

Based on the summary of the logistic regression model printed above, some predictors in the model appear to be statistically significant at at least the 5% level of significance. The predictors that are statistically significant are: `weight` (vehicle weight (lbs.)), `year` (model year (modulo 100)), and `origin` (origin of car - options include: American, European, or Japanese).

**Confusion matrix using the test data with a probability threshold set to 0.50 to determine class labels**

```{r}
test.pred.prob <- predict(glm.fit, newdata = auto_glm[-rowTrain,],
                          type = "response")

test.pred <- rep("0", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] <- "1"

confusionMatrix(data = as.factor(test.pred),
                reference = as.factor(auto_glm$mpg_cat[-rowTrain]),
                positive = "1")
```

The confusion matrix is showing that the logistic regression model accurately predicted 51 of the data points as having low gas mileage and 51 of the data points as having high gas mileage. However, the logistic regression model incorrectly predicted 7 data points with low gas mileage as having high gas mileage, and 7 data points with high gas mileage as having low gas mileage. The resulting prediction accuracy is 87.93% (95% CI: 0.8058, 0.9324), with a No Information Rate (NIR) of 0.5. The kappa statistic takes into account the possibility of agreement by random chance. The kappa statistic of 0.7586 is closer to 1 (complete agreement) than to 0 (agreement by chance). It is also greater than a cutoff value of 0.6, indicating substantial agreement. The proportion of true positives in the positive observations (sensitivity) is the same as the proportion of true negatives in the negative observations (specificity). The value of 0.8793 for sensitivity and specificity is high, as it is closer to 1 than 0. Additionally, the PPV and NPV are equal and high, at a value of 0.8793 as well. 

# (b) Train a multivariate adaptive regression spline (MARS) model using the training data.

```{r}
set.seed(1)

ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

mars.fit <- train(x = auto[rowTrain,1:7],
                    y = as.factor(auto$mpg_cat[rowTrain]),
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:4,
                                           nprune = 2:20),
                    metric = "ROC",
                    trControl = ctrl)

# plot(mars.fit)
```

# (c) Perform LDA using the training data.

```{r}
set.seed(1)

lda.fit <- lda(mpg_cat~., data = auto,
               subset = rowTrain)

lda.pred <- predict(lda.fit, newdata = auto[-rowTrain,])
head(lda.pred$posterior)
```

## Plot of the linear discriminants in LDA

```{r}
plot(lda.fit)
```

# (d) Prediction of the response variable.

## Method 1:

First, fit the GLM and LDA models using the caret package (note that the MARS model was already fit using the caret package):

```{r}
set.seed(1)

# fit the GLM model using the training dataset
glm.fit <- train(x = auto[rowTrain,1:7],
                 y = as.factor(auto$mpg_cat[rowTrain]),
                 method = "glm",
                 metric = "ROC",
                 trControl = ctrl) # 10-fold CV

# fit the LDA model using the training dataset
lda.fit <- train(x = auto[rowTrain,1:7],
                y = as.factor(auto$mpg_cat[rowTrain]),
                method = "lda",
                metric = "ROC",
                trControl = ctrl) # 10-fold CV
```

Then, apply the `predict()` function in the caret package to each model fit using the test dataset:

```{r}
set.seed(1)

glm.pred <- predict(glm.fit, newdata = auto[-rowTrain,], type = "prob")[,2]
mars.pred <- predict(mars.fit, newdata = auto[-rowTrain,], type = "prob")[,2]
lda.pred <- predict(lda.fit, newdata = auto[-rowTrain,], type = "prob")[,2]

roc.glm <- roc(auto$mpg_cat[-rowTrain], glm.pred)
roc.mars <- roc(auto$mpg_cat[-rowTrain], mars.pred)
roc.lda <- roc(auto$mpg_cat[-rowTrain], lda.pred)

auc <- c(roc.glm$auc[1], roc.mars$auc[1], roc.lda$auc[1])

modelNames <- c("glm", "mars", "lda")

ggroc(list(roc.glm, roc.mars, roc.lda), legacy.axes = TRUE) +
scale_color_discrete(labels = paste0(modelNames, " (", round(auc,3),")"),
name = "Models (AUC)") +
geom_abline(intercept = 0, slope = 1, color = "grey")
```

Using this method, I would use the GLM model to predict the response variable, `mpg_cat` because it has the highest value for area under the curve (AUC), which indicates best performance. 

## Method 2

Fit GLM, MARS, and LDA models using the test dataset, print ROC value(s), and use the `resamples()` function to calculate the mean AUC value for each model:

```{r}
set.seed(1)

# GLM
model.glm_test <- train(x = auto[-rowTrain,1:7], # test dataset
                   y = as.factor(auto$mpg_cat[-rowTrain]),
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl) # 10-fold CV

# MARS
model.mars_test <- train(x = auto[-rowTrain,1:7], # test dataset
                    y = as.factor(auto$mpg_cat[-rowTrain]),
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:4,
                                           nprune = 2:20),
                    metric = "ROC",
                    trControl = ctrl) # 10-fold CV

# LDA
model.lda_test <- train(x = auto[-rowTrain,1:7], # test dataset
                   y = as.factor(auto$mpg_cat[-rowTrain]),
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl) # 10-fold CV

model.glm_test$results$ROC # print ROC value for GLM model
model.mars_test$results$ROC # print ROC values for MARS model
model.lda_test$results$ROC # print ROC value for LDA model

res <- resamples(list(GLM = model.glm_test,
                      MARS = model.mars_test,
                      LDA = model.lda_test))
summary(res)
```

Using this method, I would use the MARS model to predict the response variable, `mpg_cat` because it has the highest value for ROC/AUC.

## Plot of ROC curve using the test data for the GLM model and misclassification error rate. 

```{r}
set.seed(1)

# plot the ROC curve
roc_glm <- roc(auto$mpg_cat[-rowTrain], test.pred.prob)
plot(roc_glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_glm), col = 4, add = TRUE)


# compute the confusion matrix and misclassification error rate
cm <- confusionMatrix(data = as.factor(test.pred),
                reference = as.factor(auto_glm$mpg_cat[-rowTrain]),
                positive = "1")

(1 - (cm$overall["Accuracy"])) # misclassification error rate = 1 - accuracy 
```

The AUC for the GLM model is 0.963, indicating that the prediction performance is very good. The misclassification error rate (1 - accuracy) is 0.1206897 or about 12.07%.